{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2464, 79)\n",
      "(2464, 1)\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "\n",
    "X = pd.read_csv('newdata.txt',header=None,sep=\" \")\n",
    "Y = pd.read_csv('newlabels.txt',header=None, sep=\" \")\n",
    "X=X.iloc[:,2:]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling data\n",
    "\n",
    "#idx = np.random.permutation(X.index)\n",
    "#X = X.reindex(idx)\n",
    "#Y = Y.reindex(idx)\n",
    "#Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1972, 79)\n",
      "(493, 79)\n",
      "(1972, 1)\n",
      "(493, 1)\n"
     ]
    }
   ],
   "source": [
    "# spliiting training and  testing data\n",
    "n=int(X.shape[0]*0.80)\n",
    "length=X.shape[0]\n",
    "X_train=X.loc[0:n,:]\n",
    "X_test1=X.loc[n:,:]\n",
    "\n",
    "Y_train=Y.loc[0:n]\n",
    "Y_test1=Y.loc[n:]\n",
    "\n",
    "X=X_train\n",
    "Y=Y_train\n",
    "print(X.shape)\n",
    "print(X_test1.shape)\n",
    "print(Y.shape)\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "accuracy=[]\n",
    "f1=[]\n",
    "precision=[]\n",
    "recall=[]\n",
    "\n",
    "f1_class=np.empty((0,7))\n",
    "precision_class=np.empty((0,7))\n",
    "recall_class=np.empty((0,7))\n",
    "# X=np.loadtxt('newdata.txt')\n",
    "# Y=np.loadtxt('newlabels.txt')\n",
    "# Y=Y.astype(int)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    \n",
    "    smote_tomek = SMOTETomek(random_state=0)\n",
    "    X_under, y_under=smote_tomek.fit_resample(X_train, y_train) #sampling on the trainng data only\n",
    "    X_under,y_under= shuffle(X_under,y_under)\n",
    "    \n",
    "    \n",
    "    clf = SVC(kernel='rbf',gamma='scale',decision_function_shape='ovo')\n",
    "    clf.fit(X_under, y_under)\n",
    "    predict_class = clf.predict(X_test)\n",
    "    \n",
    "       #f1 values\n",
    "    f1_value=metrics.f1_score(y_test,predict_class,average='macro')\n",
    "    f1_class_value=metrics.f1_score(predict_class,y_test,average=None).reshape(1, -1)\n",
    "    \n",
    "    #precision values\n",
    "    precision_value=metrics.precision_score(predict_class,y_test,average='macro')\n",
    "    precision_class_value=metrics.precision_score(predict_class,y_test,average=None).reshape(1, -1)\n",
    "    \n",
    "    #recall values\n",
    "    recall_value=metrics.recall_score(predict_class,y_test,average='macro')\n",
    "    recall_class_value=metrics.recall_score(predict_class,y_test,average=None).reshape(1, -1)\n",
    "    \n",
    "#     scores = model.evaluate(X_train[test], newY_train[test], verbose=0)\n",
    "#     accuracy.append(scores[1])\n",
    "    \n",
    "    f1.append(f1_value)\n",
    "    precision.append(precision_value)\n",
    "    recall.append(recall_value)\n",
    "    \n",
    "    f1_class=np.concatenate((f1_class,f1_class_value),axis=0)\n",
    "#     print(f1_class)\n",
    "    precision_class=np.concatenate((precision_class,precision_class_value),axis=0)\n",
    "    recall_class=np.concatenate((recall_class,recall_class_value),axis=0)\n",
    "    \n",
    "average_f1=f1_class.mean(axis=0)\n",
    "average_precision=precision_class.mean(axis=0)\n",
    "average_recall=recall_class.mean(axis=0)\n",
    "#     f1.append(metrics.f1_score(predictions,y_test,average='macro'))\n",
    "#     prs.append(metrics.precision_score(predictions,y_test,average='macro'))\n",
    "#     recall.append\n",
    "#     print(clf)\n",
    "#     print(y_test)\n",
    "#     print(clf.predict(X_test))\n",
    "#     print(clf.decision_function(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "F1 is :0.6627393588798733\n",
      "Precision is: 0.6679369035401874\n",
      "Recall is: 0.6810361051399904\n",
      "\n",
      "\n",
      "Average F1 for all the classes is:[0.98102939 0.26666667 0.70597403 0.95943705 0.71273504 0.93333333\n",
      " 0.08      ]\n",
      "Average F1 for all the classes is:[0.97994429 0.2        0.75       0.97894737 0.76666667 0.9\n",
      " 0.1       ]\n",
      "Average F1 for all the classes is:[0.98215135 0.4        0.69428571 0.94129187 0.68285714 1.\n",
      " 0.06666667]\n"
     ]
    }
   ],
   "source": [
    "print(\"For training:\")\n",
    "print(\"F1 is :\"+ str(np.mean(f1)))\n",
    "print(\"Precision is: \"+str(np.mean(precision)))\n",
    "print(\"Recall is: \"+str(np.mean(recall)))\n",
    "print(\"\\n\")\n",
    "print(\"Average F1 for all the classes is:\" +str(average_f1))\n",
    "print(\"Average F1 for all the classes is:\" +str(average_precision))\n",
    "print(\"Average F1 for all the classes is:\" +str(average_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class = clf.predict(X_test1)\n",
    "\n",
    "#f1 values\n",
    "f1_value=metrics.f1_score(Y_test1,predict_class,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test set\n",
      "F1 is :0.6627393588798733\n"
     ]
    }
   ],
   "source": [
    "print(\"For test set\")\n",
    "print(\"F1 is :\"+ str(np.mean(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=[]\n",
    "f1=[]\n",
    "precision=[]\n",
    "recall=[]\n",
    "\n",
    "f1_class=np.empty((0,7))\n",
    "precision_class=np.empty((0,7))\n",
    "recall_class=np.empty((0,7))\n",
    "# X=np.loadtxt('newdata.txt')\n",
    "# Y=np.loadtxt('newlabels.txt')\n",
    "# Y=Y.astype(int)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    \n",
    "    smote_tomek = SMOTETomek(random_state=0)\n",
    "    X_under, y_under=smote_tomek.fit_resample(X_train, y_train) #sampling on the trainng data only\n",
    "    X_under,y_under= shuffle(X_under,y_under)\n",
    "    \n",
    "    \n",
    "    clf=KNeighborsClassifier(n_neighbors=3)\n",
    "    clf.fit(X_under, y_under)\n",
    "    predict_class = clf.predict(X_test)\n",
    "    \n",
    "       #f1 values\n",
    "    f1_value=metrics.f1_score(predict_class,y_test,average='macro')\n",
    "    f1_class_value=metrics.f1_score(predict_class,y_test,average=None).reshape(1, -1)\n",
    "    \n",
    "    #precision values\n",
    "    precision_value=metrics.precision_score(predict_class,y_test,average='macro')\n",
    "    precision_class_value=metrics.precision_score(predict_class,y_test,average=None).reshape(1, -1)\n",
    "    \n",
    "    #recall values\n",
    "    recall_value=metrics.recall_score(predict_class,y_test,average='macro')\n",
    "    recall_class_value=metrics.recall_score(predict_class,y_test,average=None).reshape(1, -1)\n",
    "    \n",
    "#     scores = model.evaluate(X_train[test], newY_train[test], verbose=0)\n",
    "#     accuracy.append(scores[1])\n",
    "    \n",
    "    f1.append(f1_value)\n",
    "    precision.append(precision_value)\n",
    "    recall.append(recall_value)\n",
    "    \n",
    "    f1_class=np.concatenate((f1_class,f1_class_value),axis=0)\n",
    "#     print(f1_class)\n",
    "    precision_class=np.concatenate((precision_class,precision_class_value),axis=0)\n",
    "    recall_class=np.concatenate((recall_class,recall_class_value),axis=0)\n",
    "    \n",
    "average_f1=f1_class.mean(axis=0)\n",
    "average_precision=precision_class.mean(axis=0)\n",
    "average_recall=recall_class.mean(axis=0)\n",
    "#     f1.append(metrics.f1_score(predictions,y_test,average='macro'))\n",
    "#     prs.append(metrics.precision_score(predictions,y_test,average='macro'))\n",
    "#     recall.append\n",
    "#     print(clf)\n",
    "#     print(y_test)\n",
    "#     print(clf.predict(X_test))\n",
    "#     print(clf.decision_function(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "Accuracy is :nan\n",
      "F1 is :0.4660067549005767\n",
      "Precision is: 0.7004464285714287\n",
      "Recall is: 0.41328215715097316\n",
      "\n",
      "\n",
      "Average F1 for all the classes is:[0.88011559 0.15515873 0.37209402 0.84349375 0.3842434  0.5952381\n",
      " 0.0317037 ]\n",
      "Average F1 for all the classes is:[0.79241071 0.36666667 0.76666667 0.99166667 0.88571429 0.83333333\n",
      " 0.26666667]\n",
      "Average F1 for all the classes is:[0.99004071 0.1021645  0.24856897 0.73938629 0.2492876  0.54666667\n",
      " 0.01686036]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(\"For training:\")\n",
    "print(\"F1 is :\"+ str(np.mean(f1)))\n",
    "print(\"Precision is: \"+str(np.mean(precision)))\n",
    "print(\"Recall is: \"+str(np.mean(recall)))\n",
    "print(\"\\n\")\n",
    "print(\"Average F1 for all the classes is:\" +str(average_f1))\n",
    "print(\"Average F1 for all the classes is:\" +str(average_precision))\n",
    "print(\"Average F1 for all the classes is:\" +str(average_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
